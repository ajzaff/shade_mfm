//include "cpu_gpu_shared.inl"
//include "uniforms.inl"
//include "defines.inl"
//include "globals.inl"
//include "prng.inl"
//include "atom_decls.inl"
//include "bit_packing.inl"
//include "sites.inl"
//include "atoms.inl"
//include "xoroshiro128starstar.inl"

bool isActiveMem(ivec2 vote_idx) {
	uint center_v = imageLoad(img_vote, vote_idx).x;
	const int R = EVENT_WINDOW_RADIUS*2;
    for (int y = -R; y <= R; ++y) {
        for (int x = -R; x <= R; ++x) {
            int m = abs(x) + abs(y);
            if (m <= R && !(x == 0 && y == 0)) {
				uint v = imageLoad(img_vote, vote_idx + ivec2(x,y)).x;
                if (v >= center_v) {
                    return false;
                }
            }
        }
    }
	return true;
}

void main() {

	if (stage == 0) {
		uvec2 size = imageSize(img_vote);
		ivec2 vote_idx = ivec2(gl_GlobalInvocationID.xy);

		if (vote_idx.x < size.x && vote_idx.y < size.y) { 
			_XORO = xoroshiro128_unpack(imageLoad(img_prng_state, vote_idx));
			uint center_v = XoroshiroNext32();
			imageStore(img_vote, vote_idx, uvec4(center_v));
			imageStore(img_prng_state, vote_idx, xoroshiro128_pack(_XORO));
		}
	} else if (stage == 1) {
		uvec2 size = imageSize(img_site_bits);
		ivec2 center_idx = ivec2(gl_GlobalInvocationID.xy);
		if (center_idx.x < size.x && center_idx.y < size.y) {		
			ivec2 vote_idx = center_idx +  ivec2(EVENT_WINDOW_RADIUS*2);			
#if 1 // I feel like this is the crux: can't have atomics inside a branch that isn't taking a dynamically uniform path (isActiveMem depends on image loads)
			if  (isActiveMem(vote_idx)) {
				uint event_job_idx = min(event_jobs_count_max-1, atomicAdd(comm.primCount, 1));
				event_jobs[event_job_idx].site_idx = center_idx;
			}
#else // This workaround doesn't work either...making comm/event_jobs coherent also doesn't appear to do anything. I'm missing something...
			bool is_active = isActiveMem(vote_idx);
			uint event_job_idx = min(event_jobs_count_max-1, atomicAdd(comm.primCount, is_active ? 1 : 0));
			if (is_active)
				event_jobs[event_job_idx].site_idx = center_idx;
#endif
		}
	} else if (stage == 2) {
		if (gl_GlobalInvocationID.x < comm.primCount && gl_GlobalInvocationID.y == 0) {
			_SITE_IDX = event_jobs[gl_GlobalInvocationID.x].site_idx;
			//ivec2 vote_idx = _SITE_IDX +  ivec2(EVENT_WINDOW_RADIUS*2);
			ivec2 vote_idx = _SITE_IDX - ivec2(EVENT_WINDOW_RADIUS);
			_XORO = xoroshiro128_unpack(imageLoad(img_prng_state, vote_idx));
			//uvec4 D = uvec4(0);
			
			Atom S = _SITE_LOAD(ivec2(0,0));

			//_BEHAVE_DISPATCH(_UNPACK_TYPE(S));
			// This simulates having atoms inside do lots of RNG rolls, but a different number for each one (based on rolls themselves)
			// Works deterministically in direct update, but here it causes total avalanche of diverging results (but interestingly only for sites > 8 or so from the edge)
			// ...which makes the indexing suspicious, but it is the same as the direct case..
			// but if you mess with the idx (-r, 0, +r), you just get more different kinds of garbage. i think the "off by 8" is a red herring.
			if (random_create(8) == 0) {
				for (int i = 0; i < 128; ++i)
					random_create(8);
			}

			//D.z = 1; // #HACK site updated signal
				
			atomicAdd(stats.event_count_this_batch, 1);
				
			uint event_count = imageLoad(img_event_count, _SITE_IDX).x;
			event_count += 1;
			imageStore(img_event_count, _SITE_IDX, uvec4(event_count));
			imageStore(img_prng_state, vote_idx, xoroshiro128_pack(_XORO));
			//imageStore(img_dev, _SITE_IDX, D);
		}
	} else if (stage == 3) { // reorder later
		if (gl_GlobalInvocationID.x == 0 && gl_GlobalInvocationID.y == 0) {
			comm.primCount = 0;
		}
	}
}